{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef39c576",
   "metadata": {},
   "source": [
    "## Importing Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9128448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualizating the data\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Processing the data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Machine Learning part\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, plot_confusion_matrix, roc_auc_score, roc_curve,auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report, log_loss, precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a850bb",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"datasets/tweet_analysis_train.csv\",encoding = \"utf-8\",engine = \"python\",header = 0)\n",
    "\n",
    "test_data = pd.read_csv(\"datasets/tweet_analysis_test.csv\",encoding = \"utf-8\",engine = \"python\",header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54de04",
   "metadata": {},
   "source": [
    "### Getting info about the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a53f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set shape : \", train_data.shape)\n",
    "print(\"Test set shape : \", test_data.shape)\n",
    "print(train_data.info())\n",
    "#train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8ae70",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the column 'id'\n",
    "train_data = train_data.drop('id', axis = 'columns')\n",
    "\n",
    "## Checkinf for duplicates\n",
    "print(\"Total duplicates in train set : \", train_data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b585fa77",
   "metadata": {},
   "source": [
    "#### Since the train data set contains duplicate we will remove that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a540ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop_duplicates(keep = 'first')\n",
    "print(\"Total duplicates in train set : \", train_data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d91a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for the Null\n",
    "train_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2de0c",
   "metadata": {},
   "source": [
    "### Now check for the count of 1 and 0 in the label and also the percentage of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331410aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zeroes = 100 * len(train_data.loc[train_data['label'] == 0,'label']) / len(train_data['label'])\n",
    "Ones = 100 * len(train_data.loc[train_data['label'] == 1,'label']) / len(train_data['label'])\n",
    "print(\"Percentage of Zeroes(Positive) Sentiment tweets is : \", Zeroes)\n",
    "print(\"Percentage of Ones(Negative) Sentiment tweets is : \", Ones)\n",
    "print(\"\\nAs we can see, The training dataset is very much Imbalanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7660f",
   "metadata": {},
   "source": [
    "### Now we will get 20 most frequent words in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cf2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Count = CountVectorizer(stop_words='english')\n",
    "words = Count.fit_transform(train_data.tweet)\n",
    "Sum = words.sum(axis = 0)\n",
    "List = [(words, Sum[0,j]) for words, j in Count.vocabulary_.items()]\n",
    "List = sorted(List, key= lambda x : x[1], reverse = True)\n",
    "Freq = pd.DataFrame(List, columns = ['word', 'frequency'])\n",
    "print(Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46edbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting a histogram to show it graphically\n",
    "Freq.head(10).plot(x = 'word', y = 'frequency', kind = 'bar', figsize = (10, 7), color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f04d9a",
   "metadata": {},
   "source": [
    "### Now we will define functions to calculate , words, characters, Hashtags, Numbers, stopwords etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Words count\n",
    "def Count_words(dataframe):\n",
    "    dataframe['words'] = dataframe['tweet'].apply(lambda x : len(str(x).split(\" \")))\n",
    "    \n",
    "## Characters Count\n",
    "def Count_chars(dataframe):\n",
    "    dataframe['characters'] = dataframe['tweet'].apply(lambda x : sum(list(map(len, x.split(' ')))))\n",
    "    \n",
    "## Hastags count\n",
    "def Count_hashtags(dataframe):\n",
    "    dataframe['hashtags'] = dataframe['tweet'].apply(lambda x : len([x for x in x.split() if x.startswith('#')]))\n",
    "    \n",
    "## stopwords count\n",
    "def Count_stopwords(dataframe):\n",
    "    stopword = stopwords.words('english')\n",
    "    dataframe['stopwords'] = dataframe['tweet'].apply(lambda x : len([x for x in x.split() if x in stopword]))\n",
    "    \n",
    "## Numbers count\n",
    "def Count_words(dataframe):\n",
    "    dataframe['numbers'] = dataframe['tweet'].apply(lambda x : len([x for x in x.split() if x.isdigit()]))\n",
    "    \n",
    "    \n",
    "def Calculate(dataframe):\n",
    "    Count_words(dataframe)\n",
    "    Count_chars(dataframe)\n",
    "    Count_hashtags(dataframe)\n",
    "    Count_stopwords(dataframe)\n",
    "    Count_words(dataframe)\n",
    "\n",
    "Calculate(train_data)\n",
    "Calculate(test_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c80e93",
   "metadata": {},
   "source": [
    "### Now cleaning and processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')\n",
    "stemm = nltk.SnowballStemmer('english')\n",
    "\n",
    "def Delete(word):\n",
    "    word = str(word).lower()\n",
    "    word = re.sub('\\[.*?\\]', '', word)\n",
    "    word = re.sub('https?://\\S+|www\\.\\S+', '', word)\n",
    "    word = re.sub('<.*?>+', '', word)\n",
    "    word = re.sub('[%s]' % re.escape(string.punctuation), '', word)\n",
    "    word = re.sub('\\n', '', word)\n",
    "    word = re.sub('\\w*\\d\\w*', '', word)\n",
    "    word = [text for text in word.split(' ') if text not in stopword]\n",
    "    word =\" \".join(word)\n",
    "    word = [stemm.stem(text) for text in word.split(' ')]\n",
    "    word =\" \".join(word)\n",
    "    return word\n",
    "\n",
    "train_data['tweet'] = train_data['tweet'].apply(Delete)\n",
    "test_data['tweet'] = test_data['tweet'].apply(Delete)\n",
    "##train_data\n",
    "##Process(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca407517",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(train_data['tweet'], train_data['label'], test_size = 0.20, shuffle = True, random_state = 11)\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66efd0",
   "metadata": {},
   "source": [
    "### Now there is the need to vectorize the tweets, as this is method in Natural Language Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb590f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using TfidVectorizer\n",
    "vector = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train_vector = vector.fit_transform(train_x)\n",
    "X_test_vector = vector.transform(test_x)\n",
    "\n",
    "print(X_train_vector.shape, X_test_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0551846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing the Imbalance of the the dataset\n",
    "plt.pie(train_y.value_counts(), labels=['Label 0 (Positive Tweets)', 'Label 1 (Negative Tweets)'], autopct='%0.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304976e",
   "metadata": {},
   "source": [
    "### Since the data is highly imblanced thus oversampling is needed to balance ths dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9575b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using SMOTE Technique oversampling is done\n",
    "smote = SMOTE()\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train_vector, train_y.values)\n",
    "print(X_train_sm.shape, y_train_sm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c87163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thus Now the dataset is balanced\n",
    "plt.pie(pd.value_counts(y_train_sm), labels=['Label 0 (Positive Tweets)', 'Label 1 (Negative Tweets)'], autopct='%0.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff76ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to Train the model\n",
    "def Training(y_actual, y_predicted):\n",
    "    ## Accuracy\n",
    "    accuracy = round(accuracy_score(y_actual, y_predicted), 3)\n",
    "    ## F1 score\n",
    "    f1 = round(f1_score(y_actual, y_predicted), 3)\n",
    "    \n",
    "    print(f'Training Scores: Accuracy={accuracy}, F1-Score={f1}')\n",
    "    \n",
    "## Function to Validate the model\n",
    "def validation(y_actual, y_predicted):\n",
    "    ## Accuracy\n",
    "    accuracy = round(accuracy_score(y_actual, y_predicted), 3)\n",
    "    ## F1 score\n",
    "    f1 = round(f1_score(y_actual, y_predicted), 3)\n",
    "    \n",
    "    print(f'Validation Scores: Accuracy={accuracy}, F1-Score={f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using LinerRegression model\n",
    "Model1 = LogisticRegression()\n",
    "Model1.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "y_train_pred = Model1.predict(X_train_sm)\n",
    "y_test_pred = Model1.predict(X_test_vector)\n",
    "\n",
    "Training(y_train_sm, y_train_pred)\n",
    "validation(test_y, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fe269",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using MultinomialNB Model\n",
    "Model2 = MultinomialNB()\n",
    "Model2.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "y_train_pred = Model2.predict(X_train_sm)\n",
    "y_test_pred = Model2.predict(X_test_vector)\n",
    "\n",
    "Training(y_train_sm, y_train_pred)\n",
    "validation(test_y, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using RandomForestClassifier model\n",
    "Model3 = RandomForestClassifier()\n",
    "Model3.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "y_train_pred = Model3.predict(X_train_sm)\n",
    "y_test_pred = Model3.predict(X_test_vector)\n",
    "\n",
    "Training(y_train_sm, y_train_pred)\n",
    "validation(test_y, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70dbd3b",
   "metadata": {},
   "source": [
    "### Out of the Models tried, The RandomForestClassifier give the most accurate Result hence we will persist with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21918dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = RandomForestClassifier(criterion='entropy', max_samples=0.8, min_samples_split=10, random_state=0)\n",
    "Model.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "y_train_pred = Model.predict(X_train_sm)\n",
    "y_test_pred = Model.predict(X_test_vector)\n",
    "\n",
    "Training(y_train_sm, y_train_pred)\n",
    "validation(test_y, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5917e75",
   "metadata": {},
   "source": [
    "### Saving the Model using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(Model, 'Tweet_analysis.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8affe47b",
   "metadata": {},
   "source": [
    "### we need to load to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load('Tweet_analysis.joblib')\n",
    "Tweet = \"i Love India\"\n",
    "Tweet = Delete(Tweet)\n",
    "Tweet = vector.transform([Tweet])\n",
    "print(Model.predict(Tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087cd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
